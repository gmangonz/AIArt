{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from struct import unpack\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "import imghdr\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = glob(r'D:\\DL-CV-ML Projects\\AIART\\data\\archive\\*\\*.png')\n",
    "dataset2 = glob(r'D:\\DL-CV-ML Projects\\AIART\\data\\delaunay\\DELAUNAY\\*\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 693/28820 [00:10<12:10, 38.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered a bad img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 5642/28820 [01:40<03:10, 121.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered a bad img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28820/28820 [08:42<00:00, 55.17it/s] \n"
     ]
    }
   ],
   "source": [
    "marker_mapping = {\n",
    "    0xffd8: \"Start of Image\",\n",
    "    0xffe0: \"Application Default Header\",\n",
    "    0xffdb: \"Quantization Table\",\n",
    "    0xffc0: \"Start of Frame\",\n",
    "    0xffc4: \"Define Huffman Table\",\n",
    "    0xffda: \"Start of Scan\",\n",
    "    0xffd9: \"End of Image\"\n",
    "}\n",
    "\n",
    "\n",
    "class JPEG:\n",
    "    def __init__(self, image_file):\n",
    "        with open(image_file, 'rb') as f:\n",
    "            self.img_data = f.read()\n",
    "    \n",
    "    def decode(self):\n",
    "        data = self.img_data\n",
    "        while(True):\n",
    "            marker, = unpack(\">H\", data[0:2])\n",
    "\n",
    "            if marker == 0xffd8:\n",
    "                data = data[2:]\n",
    "            elif marker == 0xffd9:\n",
    "                return\n",
    "            elif marker == 0xffda:\n",
    "                data = data[-2:]\n",
    "            else:\n",
    "                lenchunk, = unpack(\">H\", data[2:4])\n",
    "                data = data[2+lenchunk:]            \n",
    "            if len(data)==0:\n",
    "                break        \n",
    "\n",
    "\n",
    "bads = []\n",
    "\n",
    "for img in tqdm(dataset1):\n",
    "  # image = os.path.join(root_img, img)\n",
    "  image = JPEG(img) \n",
    "  try:\n",
    "    image.decode()   \n",
    "  except:\n",
    "    bads.append(img)\n",
    "    print('Encountered a bad img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in bads:\n",
    "  os.remove(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def int64_list_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_list_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def float_list_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_tf_records(image_dir, output_path):\n",
    "\n",
    "    if not os.path.isfile(output_path):\n",
    "      \n",
    "      print(f'Saving output to: {output_path}')\n",
    "\n",
    "      images = sorted(image_dir, key=os.path.getsize)\n",
    "      expected_size = sum(list(map(lambda x: os.path.getsize(x), images))) / (1e+9)\n",
    "\n",
    "      print(f'Writing {len(images)} to TFRecord file, expected size is {expected_size} GB ...')\n",
    "\n",
    "      writer = tf.io.TFRecordWriter(output_path)\n",
    "\n",
    "      for image_path in tqdm(images):\n",
    "\n",
    "          with tf.io.gfile.GFile(image_path, 'rb') as f:\n",
    "            try:\n",
    "                encoded_image_data = f.read()\n",
    "            except:\n",
    "                print(f'{image_path} gives Error')\n",
    "                break\n",
    "\n",
    "          # Get image width and height\n",
    "          try:\n",
    "              image = Image.open(image_path)\n",
    "              width, height = image.size\n",
    "          except:\n",
    "              print(f'{image_path} gives Error')\n",
    "              break\n",
    "\n",
    "          # Create tf.train.Example\n",
    "          tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "              'image/height': int64_feature(height),\n",
    "              'image/width': int64_feature(width),\n",
    "              'image/encoded': bytes_feature(encoded_image_data),\n",
    "          }))\n",
    "          writer.write(tf_example.SerializeToString())\n",
    "\n",
    "      writer.close()\n",
    "\n",
    "      print(\"TFRecord files created successfully!\")\n",
    "    else:\n",
    "      print(f'Skipping, {output_path} aleady exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to: D:\\DL-CV-ML Projects\\AIART\\data\\dataset1\n",
      "Writing 28818 to TFRecord file, expected size is 22.687575678 GB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28818/28818 [04:27<00:00, 107.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord files created successfully!\n"
     ]
    }
   ],
   "source": [
    "output = os.path.join('D:\\DL-CV-ML Projects\\AIART\\data', 'dataset1')\n",
    "create_tf_records(dataset1, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to: D:\\DL-CV-ML Projects\\AIART\\data\\dataset2\n",
      "Writing 11493 to TFRecord file, expected size is 3.400833821 GB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11493/11493 [00:23<00:00, 486.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord files created successfully!\n"
     ]
    }
   ],
   "source": [
    "output = os.path.join('D:\\DL-CV-ML Projects\\AIART\\data', 'dataset2')\n",
    "create_tf_records(dataset2, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
